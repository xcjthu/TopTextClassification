[train]
batch_size = 1
type_of_loss = cross_entropy_loss

optimizer = bert_adam
learning_rate = 1e-5

weight_decay = 0
momentum = 1

gamma = 0.95
step_size = 1

pre_train = 0

epoch = 64

[model]
name = SFKS_bert

hidden_size = 150
dropout = 0.2

output_dim = 4
bert_path = /data2/private/zhx/bert/chinese/

rank_method = all

[reader]
max_queue_size = 40
train_reader_num = 1
valid_reader_num = 1

[data]
formatter = SFKS_bert

train_data_path = /data2/private/zhx/law/SFKS/data/origin_data/final2
train_file_list = 0_train.json
valid_data_path = /data2/private/zhx/law/SFKS/data/origin_data/final2
valid_file_list = 0_test.json

need_word2vec = False

max_len1 = 64
max_len2 = 192

multi_choice = True

shuffle_option = False

topk = 18


[output]
model_path = /data2/private/zhx/law/SFKS/model
model_name = bert_typea_all_all
tensorboard_path = /data2/private/zhx/law/SFKS/tensorboard
test_time = 1
output_time = 1
