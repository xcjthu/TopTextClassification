[train]
batch_size = 128
type_of_loss = cross_entropy_loss

optimizer = adam
learning_rate =	1e-5

weight_decay = 1e-4

momentum = 1

gamma = 0.95
step_size = 1

pre_train = 0

epoch = 1024


[model]
name = DataMining

hidden_size = 128
filter = 64

# encoder = textcnn
# encoder = lstm
encoder = bert

min_gram = 2
max_gram = 5

num_layers = 1

bert_path = /data/disk3/private/zhx/bert/chinese/

filters = 32

[reader]
max_queue_size = 20
train_reader_num = 1
valid_reader_num = 1

[data]
formatter = DataMining

train_data_path = /data/disk1/private/xcj/LAPP/DataMining/data
# train_file_list = train.json
train_file_list = train.small.json

valid_data_path = /data/disk1/private/xcj/LAPP/DataMining/data
# valid_file_list = valid.json
valid_file_list = valid.small.json

word2id = /data/disk1/private/xcj/LAPP/data/word2id.txt

need_word2vec = False

max_len = 35


[output]
model_path = /data/disk1/private/xcj/LAPP/model
model_name = DataMining
tensorboard_path = /data/disk1/private/xcj/LAPP/tensorboard
test_time = 1
output_time = 1


