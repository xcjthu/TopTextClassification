[train]
batch_size = 16
type_of_loss = cross_entropy_loss

optimizer = bert_adam
learning_rate = 1e-5

weight_decay = 1e-3

gamma = 1

pre_train = 0

epoch = 64

[model]
name = LJPBert

bert_path = /data/disk1/private/zhx/bert/xs


[reader]
max_queue_size = 40
train_reader_num = 1
valid_reader_num = 1


[data]
formatter = LJPBertFormatter

train_data_path = /data/disk1/private/zhx/ljp/data/pku
train_file_list = 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14
valid_data_path = /data/disk1/private/zhx/ljp/data/pku
valid_file_list = 15,16,17,18,10

crit_label = /data/disk1/private/zhx/ljp/data/pku/crit.txt
law_label = /data/disk1/private/zhx/ljp/data/pku/law.txt
task = crit

max_len = 512

need_word2vec = False

min_freq = 100


[output]
model_path = /data/disk5/private/zhx/ljp/model
model_name = pku_chinese
tensorboard_path = /data/disk5/private/zhx/ljp/tensorboard
test_time = 1
output_time = 1
which = F1